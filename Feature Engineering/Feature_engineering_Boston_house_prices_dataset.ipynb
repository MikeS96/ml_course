{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature engineering - Boston house prices dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PsABsDvqFj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "73203925-f954-4e12-90fc-6104fe064638"
      },
      "source": [
        "!pip install mglearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mglearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/38/8aced26fce0b2ae82c3c87cd3b6105f38ca6d9d51704ecc44aa54473e6b9/mglearn-0.1.9.tar.gz (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from mglearn) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.0.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from mglearn) (7.0.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from mglearn) (2.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.4.7)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->mglearn) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mglearn) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler->mglearn) (1.15.0)\n",
            "Building wheels for collected packages: mglearn\n",
            "  Building wheel for mglearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582639 sha256=f165ce6ad488baf7118d541f18c0fdd1676fb13ad2ad9ec9563311c2bd385601\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/a6/ea/a6a3716233fa62fc561259b5cb1e28f79e9ff3592c0adac5f0\n",
            "Successfully built mglearn\n",
            "Installing collected packages: mglearn\n",
            "Successfully installed mglearn-0.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnPeY9twrRiV",
        "colab_type": "text"
      },
      "source": [
        "## Dependencias del sistema "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGLon9FYqK2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies\n",
        "import numpy as np\n",
        "import mglearn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeXNvTlprrNo",
        "colab_type": "text"
      },
      "source": [
        "La siguientes lineas de código importan el Boston House Prices dataset with Sklearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aSwvfmIrngz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "223bae70-c061-4504-e824-3c1af35b4284"
      },
      "source": [
        "# Importing the dataset\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Creating an object with the dataset\n",
        "boston = load_boston()\n",
        "\n",
        "# Printing the keys of the dataset\n",
        "print('The Boston keys are: {}'.format(boston.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Boston keys are: dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehrrqHj6sWVV",
        "colab_type": "text"
      },
      "source": [
        "### Exploración del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJHuUwmir4cp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9d2ac48-fdb2-496a-ad47-d7d68b9ab2e4"
      },
      "source": [
        "# Printing the shape of the features\n",
        "print('The Boston dataset features have shape: {} \\n'.format(boston.data.shape))\n",
        "# Printing the first sample of features \n",
        "print('The first row of features is: {} \\n'.format(boston.data[0, :]))\n",
        "# Printing the shape of the features\n",
        "print('The Boston dataset targets have shape: {} \\n'.format(boston.target.shape))\n",
        "# Printing the first sample target\n",
        "print('The first row of targets is: {} \\n'.format(boston.target[0]))\n",
        "# Printing the shape of the features\n",
        "print('The Boston dataset feature names are: {} \\n'.format(boston.feature_names))\n",
        "# Printing the shape of the features\n",
        "print('The Boston dataset description is: {} \\n'.format(boston.DESCR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Boston dataset features have shape: (506, 13) \n",
            "\n",
            "The first row of features is: [6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
            " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00] \n",
            "\n",
            "The Boston dataset targets have shape: (506,) \n",
            "\n",
            "The first row of targets is: 24.0 \n",
            "\n",
            "The Boston dataset feature names are: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT'] \n",
            "\n",
            "The Boston dataset description is: .. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwaQnO_nvAoa",
        "colab_type": "text"
      },
      "source": [
        "De la anterior celda se puede concluír que el dataset ya ha sido procesado y cuenta con 13 variables numéricas (features) y un target (Median value of owner-occupied homes in $1000's). Sin embargo, cabe resaltar que la variable 'CHAS' es una variable dummy, donde según la descripción del dataset: **1 if tract bounds river; 0 otherwise**. Con esto en mente significa que la variable **CHAS** fue categórica en algún momento, le realizaron one-hot encoding y usaron una feature de este one-hot encoding como dummy variable. \n",
        "\n",
        "Cabe resaltar que para considerar one-hot encoding en una variable como **CHAS**, deberian haber dos dummy variables, una que sea 1 cuando tract bounds river, y otra que sea 1 cuando no haya tract bounds river.\n",
        "\n",
        "### 1. Entrenando el modelo con las características originales\n",
        "\n",
        "Primero se realizara el train-test-split al dataset original con un test size correspodiente al 25% del tamaño original del mismo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIp-V4RVsg3l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "412ea6f1-a265-4c44-fdf3-bc55086ce149"
      },
      "source": [
        "# Splitting the dataset and training the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size = 0.25, random_state = 0)  \n",
        "# Printing the training and test sets\n",
        "print('Training feature matrix size: ', X_train.shape)\n",
        "print('Test feature matrix size: ', X_test.shape)\n",
        "print('Training target vector size: ', y_train.shape)\n",
        "print('Test target vector size: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training feature matrix size:  (379, 13)\n",
            "Test feature matrix size:  (127, 13)\n",
            "Training target vector size:  (379,)\n",
            "Test target vector size:  (127,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUK0-lywxw29",
        "colab_type": "text"
      },
      "source": [
        "A continuación se crea un objeto del tipo Linear Regression y se entrena sobre el Boston dataset original. Adicionalmente, se validará el test accuracy train accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEhlCNc3xoJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df809e18-cc54-4456-cb46-5dff0794c542"
      },
      "source": [
        "# Linear regression training\n",
        "lr = LinearRegression().fit(X_train, y_train)\n",
        "# Validating accuracy in training and test set\n",
        "print('Training set score is: {:.2f}'.format(lr.score(X_train, y_train)))\n",
        "print('Test set score is: {:.2f}'.format(lr.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score is: 0.77\n",
            "Test set score is: 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tM35xjiyjL1",
        "colab_type": "text"
      },
      "source": [
        "Del anteriore resultado se puede concluír que la Linear Regression tiene un accuracy del 77% para el dataset de training y un 64% para el dataset de test.\n",
        "\n",
        "### 2. Entrenando Con las características escaladas y proyectadas sobre los dos primeros componentes principales\n",
        "\n",
        "Aquí se utilizará PCA para computar las dos características principales del dataset y se probaráá realizando 3 diferentes escalados sobre los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMnahbURydZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f7075db5-9feb-48de-ea46-0591f0dfed2b"
      },
      "source": [
        "# Importing dependencies\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Creating the StandardScaler object\n",
        "scaler = StandardScaler() \n",
        "# Fitting the object\n",
        "scaler.fit(X_train)\n",
        "# Scaling training and test sets\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# keep the first two principal components of the data\n",
        "pca = PCA(n_components = 2) \n",
        "# fit PCA model to boston \n",
        "pca.fit(X_train_scaled) \n",
        "\n",
        "# transform data onto the first two principal components \n",
        "X_train_pca = pca.transform(X_train_scaled) \n",
        "X_test_pca = pca.transform(X_test_scaled) \n",
        "print(\"Original shape: {}\".format(str(X_train_scaled.shape)))\n",
        "print(\"Reduced shape into two first components: {}\".format(str(X_train_pca.shape)))\n",
        "\n",
        "# There are two values in componentes_ (per feature), the first principal component and the second principal component\n",
        "print('PCA component: {}\\n'.format(pca.components_))\n",
        "\n",
        "# Linear regression training\n",
        "lr = LinearRegression().fit(X_train_pca, y_train)\n",
        "# Validating accuracy in training and test set with first principal components\n",
        "print('Training set score with StandardScaler with 2 principal components is: {:.2f}'.format(lr.score(X_train_pca, y_train)))\n",
        "print('Test set score with StandardScaler with 2 principal components is: {:.2f}'.format(lr.score(X_test_pca, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape: (379, 13)\n",
            "Reduced shape into two first components: (379, 2)\n",
            "PCA component: [[ 0.245409   -0.26116005  0.34654086  0.00343955  0.34827814 -0.18649556\n",
            "   0.31671673 -0.32059234  0.31593001  0.33986327  0.19743481 -0.19578671\n",
            "   0.31570319]\n",
            " [-0.31315442 -0.33330907  0.09242845  0.40974698  0.1943875   0.11215578\n",
            "   0.32334691 -0.36762865 -0.29299893 -0.24934481 -0.31989514  0.26583174\n",
            "  -0.05756448]]\n",
            "\n",
            "Training set score with StandardScaler with 2 principal components is: 0.50\n",
            "Test set score with StandardScaler with 2 principal components is: 0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMKPv68B19rK",
        "colab_type": "text"
      },
      "source": [
        "El anterior resultado nos permite concluir que escalando las características con Standard Scaler y utilizando solo las dos primeras componentes obtenidas con PCA el accuracy de un regresor línear para training es del 50% y para test del 25%. Siendo este un peor resultado que usando las características originales.\n",
        "\n",
        "**Utilizando MinMaxScaler y las dos componentes principales**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQXzaXztznvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8cf6e84a-d5df-4e2a-a9a6-d8a63da02305"
      },
      "source": [
        "# Creating the MinMaxScaler object\n",
        "scaler = MinMaxScaler() \n",
        "# Fitting the object\n",
        "scaler.fit(X_train)\n",
        "# Scaling training and test sets\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# keep the first two principal components of the data\n",
        "pca = PCA(n_components = 2) \n",
        "# fit PCA model to boston dataset \n",
        "pca.fit(X_train_scaled) \n",
        "\n",
        "# transform data onto the first two principal components \n",
        "X_train_pca = pca.transform(X_train_scaled) \n",
        "X_test_pca = pca.transform(X_test_scaled) \n",
        "print(\"Original shape: {}\".format(str(X_train_scaled.shape)))\n",
        "print(\"Reduced shape into two first components: {}\".format(str(X_train_pca.shape)))\n",
        "\n",
        "# There are two values in componentes_ (per feature), the first principal component and the second principal component\n",
        "print('PCA component: {}\\n'.format(pca.components_))\n",
        "\n",
        "# Linear regression training\n",
        "lr = LinearRegression().fit(X_train_pca, y_train)\n",
        "# Validating accuracy in training and test set with first principal components\n",
        "print('Training set score with StandardScaler with 2 principal components is: {:.2f}'.format(lr.score(X_train_pca, y_train)))\n",
        "print('Test set score with StandardScaler with 2 principal components is: {:.2f}'.format(lr.score(X_test_pca, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape: (379, 13)\n",
            "Reduced shape into two first components: (379, 2)\n",
            "PCA component: [[ 0.08424163 -0.2122474   0.33224195  0.00597889  0.31173098 -0.07502808\n",
            "   0.33303438 -0.21935063  0.5062468   0.45914011  0.17606225 -0.16820648\n",
            "   0.2218448 ]\n",
            " [-0.06388148 -0.38941648  0.16669062  0.19699101  0.2030664  -0.06461391\n",
            "   0.482549   -0.28220485 -0.50345758 -0.33043564 -0.14139839  0.1372987\n",
            "   0.13519254]]\n",
            "\n",
            "Training set score with StandardScaler with 2 principal components is: 0.36\n",
            "Test set score with StandardScaler with 2 principal components is: 0.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcY2xHv222AU",
        "colab_type": "text"
      },
      "source": [
        "Utilizando MinMaxScaler y las dos componentes principales es incluso peor que el StandardScaler. El training accuracy es del 36% y el test de 14%.\n",
        "\n",
        "**Utilizando RobustScaler y las dos componentes principales**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PcPGGMU2zeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "1ec70757-297a-4741-e8ed-03e243071d17"
      },
      "source": [
        "# Creating the RobustScaler object\n",
        "scaler = RobustScaler() \n",
        "# Fitting the object\n",
        "scaler.fit(X_train)\n",
        "# Scaling training and test sets\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# keep the first two principal components of the data\n",
        "pca = PCA(n_components = 2) \n",
        "# fit PCA model to boston dataset  \n",
        "pca.fit(X_train_scaled) \n",
        "\n",
        "# transform data onto the first two principal components \n",
        "X_train_pca = pca.transform(X_train_scaled) \n",
        "X_test_pca = pca.transform(X_test_scaled) \n",
        "print(\"Original shape: {}\".format(str(X_train_scaled.shape)))\n",
        "print(\"Reduced shape into two first components: {}\".format(str(X_train_pca.shape)))\n",
        "\n",
        "# There are two values in componentes_ (per feature), the first principal component and the second principal component\n",
        "print('PCA component: {}\\n'.format(pca.components_))\n",
        "\n",
        "# Linear regression training\n",
        "lr = LinearRegression().fit(X_train_pca, y_train)\n",
        "# Validating accuracy in training and test set with first principal components\n",
        "print('Training set score with StandardScaler with 2 principal components is: {:.2f}'.format(lr.score(X_train_pca, y_train)))\n",
        "print('Test set score with StandardScaler with 2 principal components is: {:.2f}'.format(lr.score(X_test_pca, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape: (379, 13)\n",
            "Reduced shape into two first components: (379, 2)\n",
            "PCA component: [[ 2.37348830e-01 -5.75092135e-02  4.76208105e-02 -4.06007738e-03\n",
            "   6.18421702e-02 -3.39591073e-02  4.05595927e-02 -5.00093078e-02\n",
            "   4.86707166e-02  4.82761272e-02  3.52268340e-02 -9.58221541e-01\n",
            "   6.94365646e-02]\n",
            " [ 9.07924411e-01 -1.34942190e-01  8.20928264e-02 -7.74041764e-04\n",
            "   1.00763826e-01 -8.38952119e-02  8.58824990e-02 -1.09901604e-01\n",
            "   8.90580238e-02  8.54585275e-02  9.54035376e-02  2.76593254e-01\n",
            "   1.15056341e-01]]\n",
            "\n",
            "Training set score with StandardScaler with 2 principal components is: 0.29\n",
            "Test set score with StandardScaler with 2 principal components is: 0.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbh5WqMf3LjM",
        "colab_type": "text"
      },
      "source": [
        "De lo anterior se puede concluír que el mejor escalador con las dos principales componentes es el Standard Scaler. El robust scaler solo obtuvo un train accuracy del 29% y test del 10%. Hasta este punto, el mejor regresor linear se obtiene usando los datos originales. Este comportamiento es debido a que usando PCA se pierde mucha información de las características.\n",
        "\n",
        "### 3. Usando binning y características de interacción\n",
        "\n",
        "Inicialmente se valida el modelo LinearRegression para compararlo posteriormente usando binning y caracteristicas de interacción."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPQ-0aUa3G0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4da49e59-149e-4ae0-cae2-3fd8e284be2f"
      },
      "source": [
        "reg = LinearRegression().fit(X_train, y_train) \n",
        "\n",
        "print('Training set score is: {:.2f}'.format(reg.score(X_train, y_train)))\n",
        "print('Test set score is: {:.2f}'.format(reg.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score is: 0.77\n",
            "Test set score is: 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wceNuxloepS2",
        "colab_type": "text"
      },
      "source": [
        "**Utilizando Binning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYTc5ueNjEZR",
        "colab_type": "text"
      },
      "source": [
        "Con el fin de utilizar binning para procesar los valores de datos originales que caen en un intervalo pequeño dado, un bin, se reemplazarán por un valor representativo de ese intervalo, a menudo el valor central.\n",
        "Se utilizara un valor de 10 bins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozACATUvkrKQ",
        "colab_type": "text"
      },
      "source": [
        "Inicialmente, se removera la columna 'CHAS' dado que en la descripción del dataset se menciona que esta es una columna dummy, por lo tanto no seria lo adecuado binarizarla ya que sus resultados son solo 1 o 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecvOSbKxD-SB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "78e07952-b410-4f87-8405-d1f556f33881"
      },
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "boston_no_CHAS = np.delete(boston.data, [3], axis=1)\n",
        "X_train_no_CHAS, X_test_no_CHAS, y_train, y_test = train_test_split(boston_no_CHAS, boston.target, test_size = 0.25, random_state = 0)\n",
        "\n",
        "# Creating KbinsDiscretizer\n",
        "kb = KBinsDiscretizer(n_bins=10, strategy='uniform')\n",
        "kb.fit(X_train_no_CHAS)\n",
        "\n",
        "# Transforming x_train and x_test to bin\n",
        "X_train_binned =  kb.transform(X_train_no_CHAS)\n",
        "X_test_binned = kb.transform(X_test_no_CHAS)\n",
        "\n",
        "# Train using X_train_binned\n",
        "lr = LinearRegression().fit(X_train_binned, y_train)\n",
        "\n",
        "# Validating accuracy in training and test set with binning\n",
        "print('Training set score with binned dataset is: {:.2f}'.format(lr.score(X_train_binned, y_train)))\n",
        "print('Test set score with binned dataset is: {:.2f}'.format(lr.score(X_test_binned, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score with binned dataset is: 0.90\n",
            "Test set score with binned dataset is: 0.68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlWDGzg1jF5S",
        "colab_type": "text"
      },
      "source": [
        "Los resultados de precisión para el test de entrenamiento y de de pruebas fueron de:\n",
        "Entrenamiento: 90%\n",
        "Pruebas: 68%\n",
        "\n",
        "Un comportamiento mejor al del dataset sin cambios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R3mwGYUmn8G",
        "colab_type": "text"
      },
      "source": [
        "Posteriormente, se realizara la misma prueba pero aplicando binning a todo el dataset, esto con el fin de comparar si hay algúna diferencia con respecto a excluir la columna 'CHAS' del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yDMCcCOi1A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1c07660c-fe24-4e2c-87f8-54241c2f508f"
      },
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# Creating KbinsDiscretizer\n",
        "kb = KBinsDiscretizer(n_bins=10, strategy='uniform')\n",
        "kb.fit(X_train)\n",
        "\n",
        "# Transforming x_train and x_test to bin\n",
        "X_train_binned =  kb.transform(X_train)\n",
        "X_test_binned = kb.transform(X_test)\n",
        "\n",
        "# Train using X_train_binned\n",
        "lr = LinearRegression().fit(X_train_binned, y_train)\n",
        "\n",
        "# Validating accuracy in training and test set with binning\n",
        "print('Training set score with binned dataset is: {:.2f}'.format(lr.score(X_train_binned, y_train)))\n",
        "print('Test set score with binned dataset is: {:.2f}'.format(lr.score(X_test_binned, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score with binned dataset is: 0.90\n",
            "Test set score with binned dataset is: 0.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gZ5sGGQdu4D",
        "colab_type": "text"
      },
      "source": [
        "De la anterior sección de código utilizando binning para transformar las caracterias continuas en caracteriticas categoricas se obtuvo los siguientes resultados de precisión para las pruebas con el set de entrenamiento y el set de test.\n",
        "\n",
        "Entrenamiento: 90%.\n",
        "Test: 69%\n",
        "\n",
        "Comparando este resultado con el dataset sin modificar, podemos afirmar que el dataset presenta mejor desempeño tanto para entrenamiento como para test si se utiliza una transformación con binning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cA5NsN-esj_",
        "colab_type": "text"
      },
      "source": [
        "**Utilizando caracteristicas de interacción**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h-jBpH6jIn7",
        "colab_type": "text"
      },
      "source": [
        "Para continuar, se utilizara caracteristicas de interacción uniendo el dataset con binning y el dataset normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxEC4vG-e7Ze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "557ef0d2-f761-46a7-9c22-d986d23cb731"
      },
      "source": [
        "# Create X test and train with the combination of it binned form and original form\n",
        "x_train_binned_np = X_train_binned.toarray()\n",
        "X_test_binned_np = X_test_binned.toarray()\n",
        "X_train_combined = np.hstack([X_train, x_train_binned_np])\n",
        "X_test_combined = np.hstack([X_test, X_test_binned_np])\n",
        "\n",
        "# Train using X_train_combined\n",
        "lr = LinearRegression().fit(X_train_combined, y_train)\n",
        "\n",
        "# Validating accuracy in training and test set with interaction features\n",
        "print('Training set score with interaction features is: {:.2f}'.format(lr.score(X_train_combined, y_train)))\n",
        "print('Test set score with interaction features is: {:.2f}'.format(lr.score(X_test_combined, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score with interaction features is: 0.92\n",
            "Test set score with interaction features is: 0.70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdlinFhEjHpK",
        "colab_type": "text"
      },
      "source": [
        "Se obtuvo los siguientes resultados de precisión para las pruebas con el set de entrenamiento y el set de test.\n",
        "\n",
        "Entrenamiento: 90%.\n",
        "Test: 70%\n",
        "\n",
        "Comparando este resultado con el dataset sin modificar, podemos afirmar que el dataset presenta mejor desempeño tanto para entrenamiento como para test si se utiliza una transformación con binning y caracteristicas de interacción. De lo cual no es una diferencia muy grande con respecto a solo usar el dataset con binning, debido a que por el tamaño de los arreglos no fue posible calcular el producto para mejorar el comportamiento de la regresión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCMzaeowt3YX",
        "colab_type": "text"
      },
      "source": [
        "### 4. Usando características polinómicas.\n",
        "\n",
        "Al usar ```PolynomialFeatures``` se genera una nueva matriz de características, la cual consta de todas las posibles combinaciones polinomiales de todas las características con grado menor o igual al *degree* indicado al momento de crear el objeto.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iuiHN0cxOap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import PolynomialFeatures library\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# create a PolynomialFeatures object \n",
        "poly = PolynomialFeatures(degree = 2, include_bias= False)\n",
        "# Fitting the object with train data\n",
        "poly.fit(X_train)\n",
        "\n",
        "# Transform Train data\n",
        "X_train_poly = poly.transform(X_train)\n",
        "\n",
        "# Transform Test data\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "print(\"Train set transformed shape: {}\".format(str(X_train_poly.shape)))\n",
        "print(\"Test set transformed shape: {}\".format(str(X_test_poly.shape)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKVb-lbP7z8r",
        "colab_type": "text"
      },
      "source": [
        "A medida que el *degree* escogido en el objeto ```PolynomialFeatures``` aumenta, la cantidad de caracteristicas tambien. Si el degree escogido es 1, mantiene la misma cantidad de caracteristicas que finalmente no son transformadas y por ende el resultado es el mismo a si no se usara la transformación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB5yqmebDw2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), ['age', 'hours-per-week']),\n",
        "     (\"onehot\", OneHotEncoder(sparse=False), ['workclass', 'education', 'gender', 'occupation'])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmB3s3Mb13si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Train using X_train_poly\n",
        "lr = LinearRegression().fit(X_train_poly, y_train)\n",
        "\n",
        "# Validating accuracy in training and test set with Polynomial features\n",
        "print('Training set score with Polynomial features dataset is: {:.2f}'.format(lr.score(X_train_poly, y_train)))\n",
        "print('Test set score with Polynomial features dataset is: {:.2f}'.format(lr.score(X_test_poly, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23qWR_UI8znY",
        "colab_type": "text"
      },
      "source": [
        "Dado que el valor *degree* en 1 no genera efecto alguno, y que al llevarlo a 3 el modelo demuestra un Accuracy negativo, el escogido es 2. Teniendo en cuenta esto, se entrena el modelo lineal de regresion con los datos transformados por el metodo polinomico.\n",
        "\n",
        "Al comparar este modelo con los datos del regresor que no usa información transformada (Training score: 0.77 y Test score: 0.64) se detecta que el accuracy en test ha disminuido poco pero el de train ha aumentado considerablemente lo que nos hace concluir que el modelo esta cayendo en *overfitting*, lo que indica que su capacidad de respuesta ante el dataset de entrenamiento es muy bueno pero al momento de generalizar decae en compración con datos no transformados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbhboCJL-fTy",
        "colab_type": "text"
      },
      "source": [
        "### 5. Es posible usar one-hot-encoding?\n",
        "\n",
        "Se revisa incialmente la descripción del dataset **Boston house prices** y a partir de las descripciones se puede notar que todos las caracteristicas de dicho dataset son numericas, y la unica caracteristica \"Categorica\" es binaria y se encuentra representada por 1 y 0, por lo que en una primera aproximación se podría decir que no es recomendable usar one-hot-encoding. De todas formas se imprime la primera fila de datos para observar sus valores y validar que son numericos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlB2KGErALfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Printing the shape of the features\n",
        "print('The Boston dataset feature names are: {} \\n'.format(boston.feature_names))\n",
        "print(\"Data from first row: \",boston.data[0,:])\n",
        "print('The Boston dataset description is: {} \\n'.format(boston.DESCR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM9Ky6w5EVG6",
        "colab_type": "text"
      },
      "source": [
        "Respondiendo a la pregunta especifica sobre es posible aplicar one-hot-encoding sobre este dataset, si es posible y se procede a realizarlo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFvl9LWJEc37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder \n",
        "# creating OneHotEncoder object \n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "\n",
        "ohe_data = ohe.fit_transform(boston.data)\n",
        "print(\"Size of one-hot-encoding features fro Boston dataset: \",ohe.get_feature_names().shape)\n",
        "print(\"Names for one-hot-encoding: \",ohe.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIQCokQXH739",
        "colab_type": "text"
      },
      "source": [
        "Al validar los datos arrojados se demuestra que si posible aplicarlo, pero no es recomendable dado que todos los valores allí contenidos son numericos y por ende el metodo creó 2836 diferentes caracteristicas para cada valor diferente encontrado. De esta forma cuando se tengan datos nuevos y se realice la transformación, existirian problemas si los nuevos valores numericos no encajan en alguna de las caracteristicas del dataset de entrenamiento."
      ]
    }
  ]
}